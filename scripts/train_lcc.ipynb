{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "sys.path.append('..')\n",
    "from models.unet_model import UNet\n",
    "from models.archs import NestedUNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Number of classes: 9\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = '../data_lcc'\n",
    "SAR_DIR = os.path.join(DATA_DIR, 'sar')\n",
    "MASK_DIR = os.path.join(DATA_DIR, 'ground_truth')\n",
    "WEIGHTS_DIR = '../weights'\n",
    "PREDICTIONS_DIR = '../predictions_lcc/unet'\n",
    "\n",
    "os.makedirs(WEIGHTS_DIR, exist_ok=True)\n",
    "os.makedirs(PREDICTIONS_DIR, exist_ok=True)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 100\n",
    "PATIENCE = 5\n",
    "LEARNING_RATE = 1e-4\n",
    "IMAGE_HEIGHT = 256\n",
    "IMAGE_WIDTH = 256\n",
    "N_CHANNELS = 3\n",
    "\n",
    "COLOR_MAP = {\n",
    "    (65, 155, 223): 1,   # 0x419bdf -> Water\n",
    "    (57, 125, 73): 2,    # 0x397d49 -> Trees\n",
    "    (122, 135, 198): 4,  # 0x7a87c6 -> Flooded Vegetation\n",
    "    (228, 150, 53): 5,   # 0xe49635 -> Crops\n",
    "    (196, 40, 27): 7,    # 0xc4281b -> Built Area\n",
    "    (165, 155, 143): 8,  # 0xa59b8f -> Bare Ground\n",
    "    (168, 235, 255): 9,  # 0xa8ebff -> Snow/Ice\n",
    "    (97, 97, 97): 10,    # 0x616161 -> Clouds\n",
    "    (227, 226, 195): 11, # 0xe3e2c3 -> Rangeland\n",
    "}\n",
    "\n",
    "CLASS_LABELS = [1, 2, 4, 5, 7, 8, 9, 10, 11]\n",
    "LABEL_TO_INDEX = {label: i for i, label in enumerate(CLASS_LABELS)}\n",
    "INDEX_TO_COLOR = {v: k for k, v in COLOR_MAP.items()}\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(f\"Number of classes: {len(CLASS_LABELS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LandCoverDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, image_ids, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.image_ids = image_ids\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def _rgb_to_mask(self, rgb_mask):\n",
    "        mask = np.zeros((rgb_mask.shape[0], rgb_mask.shape[1]), dtype=np.int64)\n",
    "        for color, label in COLOR_MAP.items():\n",
    "            locations = np.where(np.all(rgb_mask == color, axis=-1))\n",
    "            mask[locations] = LABEL_TO_INDEX[label]\n",
    "        return torch.from_numpy(mask)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_ids[idx]\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        mask_path = os.path.join(self.mask_dir, img_name)\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        mask_rgb = np.array(Image.open(mask_path).convert(\"RGB\"))\n",
    "\n",
    "        mask = self._rgb_to_mask(mask_rgb)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 754\n",
      "Training set size: 603\n",
      "Validation set size: 75\n",
      "Test set size: 76\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "all_files = sorted([f for f in os.listdir(SAR_DIR) if f.endswith('.png')])\n",
    "random.seed(42)\n",
    "random.shuffle(all_files)\n",
    "\n",
    "n_files = len(all_files)\n",
    "train_split = int(n_files * 0.7)\n",
    "val_split = int(n_files * 0.85)\n",
    "\n",
    "train_ids = all_files[:train_split]\n",
    "val_ids = all_files[train_split:val_split]\n",
    "test_ids = all_files[val_split:]\n",
    "\n",
    "train_dataset = LandCoverDataset(SAR_DIR, MASK_DIR, train_ids, transform=transform)\n",
    "val_dataset = LandCoverDataset(SAR_DIR, MASK_DIR, val_ids, transform=transform)\n",
    "test_dataset = LandCoverDataset(SAR_DIR, MASK_DIR, test_ids, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Total images: {n_files}\")\n",
    "print(f\"Training set size: {len(train_dataset)}\")\n",
    "print(f\"Validation set size: {len(val_dataset)}\")\n",
    "print(f\"Test set size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(loader, model, optimizer, loss_fn):\n",
    "    loop = tqdm(loader, desc=\"Training\")\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_idx, (data, targets) in enumerate(loop):\n",
    "        data = data.to(device=DEVICE)\n",
    "        targets = targets.to(device=DEVICE, dtype=torch.long)\n",
    "\n",
    "        predictions = model(data)\n",
    "        loss = loss_fn(predictions, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "        \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "def evaluate_model(loader, model, loss_fn):\n",
    "    num_correct = 0\n",
    "    num_pixels = 0\n",
    "    total_loss = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        loop = tqdm(loader, desc=\"Evaluating\")\n",
    "        for data, targets in loop:\n",
    "            data = data.to(device=DEVICE)\n",
    "            targets = targets.to(device=DEVICE, dtype=torch.long)\n",
    "            \n",
    "            predictions = model(data)\n",
    "            loss = loss_fn(predictions, targets)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            num_correct += (preds == targets).sum()\n",
    "            num_pixels += torch.numel(preds)\n",
    "            \n",
    "            loop.set_postfix(loss=loss.item(), accuracy=f\"{(num_correct/num_pixels)*100:.2f}%\")\n",
    "\n",
    "    accuracy = (num_correct / num_pixels) * 100\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "\n",
      "--- Epoch 1/100 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fabfc1d7fc0c43728334a54d8e86d0fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85819c32c9004d1eb6f311c2f252cb57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 74.18%\n",
      "Epoch 1: Train Loss=1.6886, Val Loss=1.6004, Val Accuracy=74.18%\n",
      "Validation loss improved. Model saved to ../weights/unetpp.pth\n",
      "\n",
      "--- Epoch 2/100 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40e8453e599e478ba1e38f91484b9ad3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "415e19555998420db0a189688d707273",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 79.26%\n",
      "Epoch 2: Train Loss=1.4345, Val Loss=1.3117, Val Accuracy=79.26%\n",
      "Validation loss improved. Model saved to ../weights/unetpp.pth\n",
      "\n",
      "--- Epoch 3/100 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbaa605c517b49f6a3db884f939964b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = NestedUNet(num_classes=len(CLASS_LABELS)).to(DEVICE)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "model_save_path = os.path.join(WEIGHTS_DIR, \"unetpp.pth\")\n",
    "epochs_no_improve = 0\n",
    "\n",
    "print(\"Starting training...\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\n--- Epoch {epoch+1}/{EPOCHS} ---\")\n",
    "    \n",
    "    train_loss = train_fn(train_loader, model, optimizer, loss_fn)\n",
    "    val_loss, val_accuracy = evaluate_model(val_loader, model, loss_fn)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}, Val Accuracy={val_accuracy:.2f}%\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        print(f\"Validation loss improved. Model saved to {model_save_path}\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f\"No improvement in validation loss for {epochs_no_improve} epoch(s).\")\n",
    "        if epochs_no_improve >= PATIENCE:\n",
    "            print(f\"Early stopping triggered. No improvement in {PATIENCE} epochs.\")\n",
    "            break\n",
    "\n",
    "\n",
    "print(\"\\n--- Training Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def mask_to_rgb(mask_tensor, class_map):\n",
    "    mask_np = mask_tensor.cpu().numpy()\n",
    "    rgb_image = np.zeros((mask_np.shape[0], mask_np.shape[1], 3), dtype=np.uint8)\n",
    "    \n",
    "    for class_idx, color in class_map.items():\n",
    "        original_label = CLASS_LABELS[class_idx]\n",
    "        rgb_color = INDEX_TO_COLOR[original_label]\n",
    "        rgb_image[mask_np == class_idx] = rgb_color\n",
    "        \n",
    "    return Image.fromarray(rgb_image)\n",
    "\n",
    "print(f\"Loading best model from {model_save_path}\")\n",
    "model.load_state_dict(torch.load(model_save_path))\n",
    "\n",
    "test_loss, test_accuracy = evaluate_model(test_loader, model, loss_fn)\n",
    "print(f\"\\n--- Test Set Performance ---\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Pixel Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "\n",
    "print(f\"\\nSaving test predictions to {PREDICTIONS_DIR}...\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, (x, y) in enumerate(tqdm(test_dataset, desc=\"Saving Predictions\")):\n",
    "        x = x.unsqueeze(0).to(DEVICE)\n",
    "        \n",
    "        preds = torch.argmax(model(x), dim=1).squeeze(0)\n",
    "        \n",
    "        pred_rgb = mask_to_rgb(preds, {i: v for i, v in enumerate(CLASS_LABELS)})\n",
    "        \n",
    "        original_filename = test_ids[i]\n",
    "        pred_rgb.save(os.path.join(PREDICTIONS_DIR, original_filename))\n",
    "\n",
    "print(\"Predictions saved successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
