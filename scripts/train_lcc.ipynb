{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "sys.path.append('..')\n",
    "from models.Models import NestedUNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Number of classes: 9\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = '../pauli_data_lcc'\n",
    "MODEL_NAME = 'unetpp'\n",
    "\n",
    "SAR_DIR = os.path.join(DATA_DIR, 'sar')\n",
    "MASK_DIR = os.path.join(DATA_DIR, 'ground_truth')\n",
    "WEIGHTS_DIR = '../weights'\n",
    "PREDICTIONS_DIR = os.path.join('predictions_lcc', MODEL_NAME)\n",
    "\n",
    "os.makedirs(WEIGHTS_DIR, exist_ok=True)\n",
    "os.makedirs(PREDICTIONS_DIR, exist_ok=True)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 100\n",
    "PATIENCE = 10\n",
    "LEARNING_RATE = 1e-4\n",
    "IMAGE_HEIGHT = 256\n",
    "IMAGE_WIDTH = 256\n",
    "N_CHANNELS = 3\n",
    "\n",
    "COLOR_MAP = {\n",
    "    (65, 155, 223): 1,   # 0x419bdf -> Water\n",
    "    (57, 125, 73): 2,    # 0x397d49 -> Trees\n",
    "    (122, 135, 198): 4,  # 0x7a87c6 -> Flooded Vegetation\n",
    "    (228, 150, 53): 5,   # 0xe49635 -> Crops\n",
    "    (196, 40, 27): 7,    # 0xc4281b -> Built Area\n",
    "    (165, 155, 143): 8,  # 0xa59b8f -> Bare Ground\n",
    "    (168, 235, 255): 9,  # 0xa8ebff -> Snow/Ice\n",
    "    (97, 97, 97): 10,    # 0x616161 -> Clouds\n",
    "    (227, 226, 195): 11, # 0xe3e2c3 -> Rangeland\n",
    "}\n",
    "\n",
    "CLASS_LABELS = [1, 2, 4, 5, 7, 8, 9, 10, 11]\n",
    "LABEL_TO_INDEX = {label: i for i, label in enumerate(CLASS_LABELS)}\n",
    "INDEX_TO_COLOR = {v: k for k, v in COLOR_MAP.items()}\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(f\"Number of classes: {len(CLASS_LABELS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LandCoverDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, image_ids, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.image_ids = image_ids\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def _rgb_to_mask(self, rgb_mask):\n",
    "        mask = np.zeros((rgb_mask.shape[0], rgb_mask.shape[1]), dtype=np.int64)\n",
    "        for color, label in COLOR_MAP.items():\n",
    "            locations = np.where(np.all(rgb_mask == color, axis=-1))\n",
    "            mask[locations] = LABEL_TO_INDEX[label]\n",
    "        return torch.from_numpy(mask)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_ids[idx]\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        mask_path = os.path.join(self.mask_dir, img_name)\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        mask_rgb = np.array(Image.open(mask_path).convert(\"RGB\"))\n",
    "\n",
    "        mask = self._rgb_to_mask(mask_rgb)\n",
    "\n",
    "        if self.transform:\n",
    "            image, mask = self.transform(image, mask)\n",
    "\n",
    "        return image, mask\n",
    "    \n",
    "class TrainTransform:\n",
    "    def __call__(self, image, mask):\n",
    "        if random.random() > 0.5:\n",
    "            image = TF.hflip(image)\n",
    "            mask = TF.hflip(mask)\n",
    "\n",
    "        if random.random() > 0.5:\n",
    "            image = TF.vflip(image)\n",
    "            mask = TF.vflip(mask)\n",
    "\n",
    "        angle = random.choice([0, 90, 180, 270])\n",
    "        image = TF.rotate(image, angle)\n",
    "        mask = TF.rotate(mask.unsqueeze(0), angle, interpolation=transforms.InterpolationMode.NEAREST).squeeze(0)\n",
    "\n",
    "        image = TF.to_tensor(image)\n",
    "        image = TF.normalize(image, mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "class ValTestTransform:\n",
    "    def __call__(self, image, mask):\n",
    "        image = TF.to_tensor(image)\n",
    "        image = TF.normalize(image, mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 736\n",
      "Training set size: 515\n",
      "Validation set size: 110\n",
      "Test set size: 111\n"
     ]
    }
   ],
   "source": [
    "all_files = sorted([f for f in os.listdir(SAR_DIR) if f.endswith('.png')])\n",
    "random.seed(42)\n",
    "random.shuffle(all_files)\n",
    "\n",
    "n_files = len(all_files)\n",
    "train_split = int(n_files * 0.7)\n",
    "val_split = int(n_files * 0.85)\n",
    "\n",
    "train_ids = all_files[:train_split]\n",
    "val_ids = all_files[train_split:val_split]\n",
    "test_ids = all_files[val_split:]\n",
    "\n",
    "train_dataset = LandCoverDataset(SAR_DIR, MASK_DIR, train_ids, transform=TrainTransform())\n",
    "val_dataset = LandCoverDataset(SAR_DIR, MASK_DIR, val_ids, transform=ValTestTransform())\n",
    "test_dataset = LandCoverDataset(SAR_DIR, MASK_DIR, test_ids, transform=ValTestTransform())\n",
    "\n",
    "num_workers = 4 if torch.cuda.is_available() else 0\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "print(f\"Total images: {n_files}\")\n",
    "print(f\"Training set size: {len(train_dataset)}\")\n",
    "print(f\"Validation set size: {len(val_dataset)}\")\n",
    "print(f\"Test set size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(loader, model, optimizer, loss_fn):\n",
    "    loop = tqdm(loader, desc=\"Training\")\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_idx, (data, targets) in enumerate(loop):\n",
    "        data = data.to(device=DEVICE)\n",
    "        targets = targets.to(device=DEVICE, dtype=torch.long)\n",
    "\n",
    "        predictions = model(data)\n",
    "        loss = loss_fn(predictions, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "        \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "def evaluate_model(loader, model, loss_fn):\n",
    "    num_correct = 0\n",
    "    num_pixels = 0\n",
    "    total_loss = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        loop = tqdm(loader, desc=\"Evaluating\")\n",
    "        for data, targets in loop:\n",
    "            data = data.to(device=DEVICE)\n",
    "            targets = targets.to(device=DEVICE, dtype=torch.long)\n",
    "            \n",
    "            predictions = model(data)\n",
    "            loss = loss_fn(predictions, targets)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(predictions, dim=1)\n",
    "            num_correct += (preds == targets).sum()\n",
    "            num_pixels += torch.numel(preds)\n",
    "            \n",
    "            loop.set_postfix(loss=loss.item(), accuracy=f\"{(num_correct/num_pixels)*100:.2f}%\")\n",
    "\n",
    "    accuracy = (num_correct / num_pixels) * 100\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "\n",
      "--- Epoch 1/100 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "774aec05a4c04004878a3215326b184b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model = NestedUNet(out_ch=len(CLASS_LABELS)).to(DEVICE)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "model_save_path = os.path.join(WEIGHTS_DIR, f\"{MODEL_NAME}.pth\")\n",
    "epochs_no_improve = 0\n",
    "\n",
    "print(\"Starting training...\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\n--- Epoch {epoch+1}/{EPOCHS} ---\")\n",
    "    \n",
    "    train_loss = train_fn(train_loader, model, optimizer, loss_fn)\n",
    "    val_loss, val_accuracy = evaluate_model(val_loader, model, loss_fn)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}, Val Accuracy={val_accuracy:.2f}%\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        print(f\"Validation loss improved. Model saved to {model_save_path}\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f\"No improvement in validation loss for {epochs_no_improve} epoch(s).\")\n",
    "        if epochs_no_improve >= PATIENCE:\n",
    "            print(f\"Early stopping triggered. No improvement in {PATIENCE} epochs.\")\n",
    "            break\n",
    "\n",
    "\n",
    "print(\"\\n--- Training Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading best model from ../weights/unetpp.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "025f04d8f4e641aaa0d09aba55407c6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test Set Performance ---\n",
      "Test Loss: 0.4399\n",
      "Test Pixel Accuracy: 85.43%\n",
      "\n",
      "Saving test predictions to ../predictions_lcc/unet...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7343f905e48a4e6dbde28990bfb98a0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving Predictions:   0%|          | 0/111 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved successfully.\n"
     ]
    }
   ],
   "source": [
    "def mask_to_rgb(mask_tensor, class_map):\n",
    "    mask_np = mask_tensor.cpu().numpy()\n",
    "    rgb_image = np.zeros((mask_np.shape[0], mask_np.shape[1], 3), dtype=np.uint8)\n",
    "    \n",
    "    for class_idx, color in class_map.items():\n",
    "        original_label = CLASS_LABELS[class_idx]\n",
    "        rgb_color = INDEX_TO_COLOR[original_label]\n",
    "        rgb_image[mask_np == class_idx] = rgb_color\n",
    "        \n",
    "    return Image.fromarray(rgb_image)\n",
    "\n",
    "print(f\"Loading best model from {model_save_path}\")\n",
    "model.load_state_dict(torch.load(model_save_path))\n",
    "\n",
    "test_loss, test_accuracy = evaluate_model(test_loader, model, loss_fn)\n",
    "print(f\"\\n--- Test Set Performance ---\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Pixel Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "\n",
    "print(f\"\\nSaving test predictions to {PREDICTIONS_DIR}...\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, (x, y) in enumerate(tqdm(test_dataset, desc=\"Saving Predictions\")):\n",
    "        x = x.unsqueeze(0).to(DEVICE)\n",
    "        \n",
    "        preds = torch.argmax(model(x), dim=1).squeeze(0)\n",
    "        \n",
    "        pred_rgb = mask_to_rgb(preds, {i: v for i, v in enumerate(CLASS_LABELS)})\n",
    "        \n",
    "        original_filename = test_ids[i]\n",
    "        pred_rgb.save(os.path.join(PREDICTIONS_DIR, original_filename))\n",
    "\n",
    "print(\"Predictions saved successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flood-detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
